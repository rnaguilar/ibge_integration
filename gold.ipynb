{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c19eb0",
   "metadata": {},
   "source": [
    "# Instanciar Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f9578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, from_json , explode, expr, lit, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"Ibge_silver\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc0503",
   "metadata": {},
   "source": [
    "# PANDAS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1e5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_upsert(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Função para atualizar os dados na tabela usando a lógica de upsert.\n",
    "\n",
    "    Args:\n",
    "        table (sqlalchemy.Table): Tabela alvo para upsert.\n",
    "        conn (sqlalchemy.engine.Connection): Conexão com o banco de dados.\n",
    "        keys (list): Lista das chaves da tabela.\n",
    "        data_iter (iterable): Iterável contendo dados a serem inseridos/atualizados.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        keys = ['id', 'name']\n",
    "        data_iter = [(1, 'John'), (2, 'Jane')]\n",
    "\n",
    "        # Tabela alvo 'users' com colunas 'id' e 'name'\n",
    "        users_table = sqlalchemy.Table('users', metadata, autoload_with=engine)\n",
    "\n",
    "        # Conexão com o banco de dados\n",
    "        db_conn = engine.connect()\n",
    "\n",
    "        # Chamada à função de upsert\n",
    "        call_upsert(users_table, db_conn, keys, data_iter)\n",
    "\n",
    "        # Fechar a conexão\n",
    "        db_conn.close()\n",
    "    \"\"\"\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "\n",
    "    # Construir a declaração de inserção\n",
    "    insert_statement = insert(table).values(data)\n",
    "\n",
    "    # Construir a declaração de upsert\n",
    "    upsert_statement = insert_statement.on_conflict_do_update(\n",
    "        constraint=f\"{table.name}_pkey\",\n",
    "        set_={c.key: c for c in insert_statement.excluded},\n",
    "    )\n",
    "\n",
    "    # Executar a declaração de upsert\n",
    "    conn.execute(upsert_statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e5f84",
   "metadata": {},
   "source": [
    "# SQL CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85383ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações de conexão com o PostgreSQL\n",
    "# Este item é totalmente editável, para uma conexão de sua preferência (Vamos fazer conforme o docker-compose que subimos)\n",
    "\n",
    "host = 'localhost'\n",
    "port = '8085'\n",
    "\n",
    "gold_url = f\"jdbc:postgresql://{host}:{port}/gold_data\"\n",
    "silver_url = f\"jdbc:postgresql://{host}:{port}/silver_data\"\n",
    "properties = {\"user\": \"ibge\", \"password\": \"ibge\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "\"\"\"\n",
    "Configurações de conexão com o PostgreSQL.\n",
    "\n",
    "Attributes:\n",
    "- host (str): O endereço do host do PostgreSQL.\n",
    "- port (str): A porta utilizada para a conexão.\n",
    "- gold_url (str): URL JDBC para o banco de dados 'gold_data'.\n",
    "- silver_url (str): URL JDBC para o banco de dados 'silver_data'.\n",
    "- properties (dict): Dicionário contendo informações de autenticação, como usuário e senha.\n",
    "\"\"\"\n",
    "\n",
    "# Criar um engine do SQLAlchemy\n",
    "gold_engine = create_engine(f\"postgresql://{properties['user']}:{properties['password']}@{host}:{port}/gold_data\")\n",
    "silver_engine = create_engine(f\"postgresql://{properties['user']}:{properties['password']}@{host}:{port}/silver_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb8379",
   "metadata": {},
   "source": [
    "# REIGAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e291ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceção durante o UpInsert: subject table for an INSERT, UPDATE or DELETE expected, got <pandas.io.sql.SQLTable object at 0x000002385DD5F5F0>.\n"
     ]
    }
   ],
   "source": [
    "# INSERÇÃO DA DIMENSÃO REGIÃO USANDO PANDAS\n",
    "\n",
    "# Definindo a query SQL para extrair dados da tabela pública 'regiao'\n",
    "query = 'SELECT * FROM public.regiao'\n",
    "\n",
    "# Iterando sobre os chunks do resultado da query SQL\n",
    "for data_frame in pd.read_sql(query, silver_engine, chunksize=10000):\n",
    "    try:\n",
    "        # Convertendo o chunk para DataFrame Pandas e inserindo/upserting na tabela 'regiao' do banco de dados 'gold_data'\n",
    "        pd.DataFrame(data_frame).to_sql(\n",
    "            'regiao', \n",
    "            gold_engine, \n",
    "            schema='public', \n",
    "            index=False, \n",
    "            if_exists=\"append\", \n",
    "            method=call_upsert\n",
    "        )\n",
    "        print(\"UpInsert executado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f'Exceção durante o UpInsert: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5edc0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceção durante o UpInsert: subject table for an INSERT, UPDATE or DELETE expected, got <pandas.io.sql.SQLTable object at 0x000002385B86E4B0>.\n"
     ]
    }
   ],
   "source": [
    "# INSERÇÃO DA DIMENSÃO ESTADO USANDO PANDAS\n",
    "\n",
    "# Definindo a query SQL para extrair dados da tabela pública 'estado'\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    public.estado\n",
    "\"\"\"\n",
    "\n",
    "# Iterando sobre os chunks do resultado da query SQL\n",
    "for data_frame in pd.read_sql(query, silver_engine, chunksize=10000):\n",
    "    try:\n",
    "        # Convertendo o chunk para DataFrame Pandas e inserindo/upserting na tabela 'estado' do banco de dados 'gold_data'\n",
    "        pd.DataFrame(data_frame).to_sql(\n",
    "            'estado', \n",
    "            gold_engine, \n",
    "            schema='public', \n",
    "            index=False, \n",
    "            if_exists=\"append\", \n",
    "            method=call_upsert\n",
    "        )\n",
    "        print(\"UpInsert executado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f'Exceção durante o UpInsert: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c5e09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceção durante o UpInsert: subject table for an INSERT, UPDATE or DELETE expected, got <pandas.io.sql.SQLTable object at 0x000002385DD5FF20>.\n"
     ]
    }
   ],
   "source": [
    "# INSERÇÃO DA DIMENSÃO MUNICÍPIO USANDO PANDAS\n",
    "\n",
    "# Definindo a query SQL para extrair dados da tabela pública 'municipio'\n",
    "sql_municipio = \"\"\"\n",
    "    SELECT\n",
    "        id_municipio,\n",
    "        municipio,\n",
    "        uf,\n",
    "        flag_capital\n",
    "    FROM\n",
    "        public.municipio;\n",
    "\"\"\"\n",
    "\n",
    "# Iterando sobre os chunks do resultado da query SQL\n",
    "for data_frame in pd.read_sql(sql_municipio, silver_engine, chunksize=10000):\n",
    "    try:\n",
    "        # Convertendo o chunk para DataFrame Pandas e inserindo/upserting na tabela 'municipio' do banco de dados 'gold_data'\n",
    "        pd.DataFrame(data_frame).to_sql(\n",
    "            'municipio', \n",
    "            gold_engine, \n",
    "            schema='public', \n",
    "            index=False, \n",
    "            if_exists=\"append\", \n",
    "            method=call_upsert\n",
    "        )\n",
    "        print(\"UpInsert executado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f'Exceção durante o UpInsert: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "530525b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LENDO COORDENADAS VIA DICIONÁRIO E INSERINDO NO BANCO DE DADOS\n",
    "\n",
    "# Dicionário contendo coordenadas para cada UF\n",
    "coordenadas = {\n",
    "    'AC': [-8.77, -70.55],\n",
    "    'AL': [-9.62, -36.82],\n",
    "    'AM': [-3.47, -65.10],\n",
    "    'AP': [1.41, -51.77],\n",
    "    'BA': [-13.29, -41.71],\n",
    "    'CE': [-5.20, -39.53],\n",
    "    'DF': [-15.83, -47.86],\n",
    "    'ES': [-19.19, -40.34],\n",
    "    'GO': [-15.98, -49.86],\n",
    "    'MA': [-5.42, -45.44],\n",
    "    'MT': [-12.64, -55.42],\n",
    "    'MS': [-20.51, -54.54],\n",
    "    'MG': [-18.10, -44.38],\n",
    "    'PA': [-3.79, -52.48],\n",
    "    'PB': [-7.28, -36.72],\n",
    "    'PR': [-24.89, -51.55],\n",
    "    'PE': [-8.38, -37.86],\n",
    "    'PI': [-6.60, -42.28],\n",
    "    'RJ': [-22.25, -42.66],\n",
    "    'RN': [-5.81, -36.59],\n",
    "    'RO': [-10.83, -63.34],\n",
    "    'RS': [-30.17, -53.50],\n",
    "    'RR': [1.99, -61.33],\n",
    "    'SC': [-27.45, -50.95],\n",
    "    'SE': [-10.57, -37.45],\n",
    "    'SP': [-22.19, -48.79],\n",
    "    'TO': [-9.46, -48.26]\n",
    "}\n",
    "\n",
    "# Convertendo o dicionário em DataFrame Pandas\n",
    "df = pd.DataFrame(list(coordenadas.items()), columns=['UF', 'Coords'])\n",
    "df[['lat', 'long']] = pd.DataFrame(df['Coords'].tolist(), index=df.index)\n",
    "\n",
    "# Descartando a coluna original 'Coords'\n",
    "df = df.drop('Coords', axis=1)\n",
    "\n",
    "# Inserindo os dados no banco de dados 'gold_data'\n",
    "df.to_sql(\n",
    "    'lat_long', \n",
    "    gold_engine, \n",
    "    schema='public', \n",
    "    index=False, \n",
    "    if_exists=\"replace\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c9cd3",
   "metadata": {},
   "source": [
    "# PESQUISAS (leitura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2ae729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+----------+------------+-------------------+\n",
      "| ano|municipio| idh|       pib|populacional| Faixa de População|\n",
      "+----+---------+----+----------+------------+-------------------+\n",
      "|2018|  1100015|NULL|  498980.0|       23167|Entre 11414 e 24725|\n",
      "|2018|  1100023|NULL| 2464704.0|      106168|    Maior que 24725|\n",
      "|2018|  1100031|NULL|  140503.0|        5438| Entre 5366 e 11414|\n",
      "|2018|  1100049|NULL| 2175840.0|       84813|    Maior que 24725|\n",
      "|2018|  1100056|NULL|  470647.0|       16444|Entre 11414 e 24725|\n",
      "|2018|  1100064|NULL|  330232.0|       16227|Entre 11414 e 24725|\n",
      "|2018|  1100072|NULL|  320416.0|        7567| Entre 5366 e 11414|\n",
      "|2018|  1100080|NULL|  230151.0|       17855|Entre 11414 e 24725|\n",
      "|2018|  1100098|NULL|  606072.0|       32047|    Maior que 24725|\n",
      "|2018|  1100106|NULL|  837459.0|       45783|    Maior que 24725|\n",
      "|2018|  1100114|NULL| 1484555.0|       51933|    Maior que 24725|\n",
      "|2018|  1100122|NULL| 3466810.0|      127907|    Maior que 24725|\n",
      "|2018|  1100130|NULL|  642283.0|       39097|    Maior que 24725|\n",
      "|2018|  1100148|NULL|  345779.0|       20459|Entre 11414 e 24725|\n",
      "|2018|  1100155|NULL|  807497.0|       36340|    Maior que 24725|\n",
      "|2018|  1100189|NULL| 1129770.0|       36434|    Maior que 24725|\n",
      "|2018|  1100205|0.73|1.665567E7|      519531|    Maior que 24725|\n",
      "|2018|  1100254|NULL|  418683.0|       19409|Entre 11414 e 24725|\n",
      "|2018|  1100262|NULL|  102476.0|        3723|     Entre 0 e 5366|\n",
      "|2018|  1100288|NULL| 1291476.0|       54702|    Maior que 24725|\n",
      "+----+---------+----+----------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IDH - PROCESSAMENTO E ANÁLISE\n",
    "\n",
    "# Definir a consulta SQL para obter dados relacionados ao IDH\n",
    "sql_query = \"\"\"\n",
    "    select\n",
    "        ano,\n",
    "        municipio,\n",
    "        tp.pesquisa as pesquisa,\n",
    "        valor\n",
    "    from\n",
    "        public.pesquisas p\n",
    "    left join\n",
    "        tipo_pesquisa tp \n",
    "    on\n",
    "        p.cd_pesquisa = tp.cd_pesquisa \n",
    "    where\n",
    "        p.ano in (2018, 2019, 2020, 2021)\n",
    "\"\"\"\n",
    "\n",
    "# Ler os dados do banco de dados usando Spark\n",
    "df_pesquisa = spark_session.read.jdbc(url=silver_url, table=f\"({sql_query}) as subquery\", properties=properties)\n",
    "\n",
    "# Pivotação do DataFrame para facilitar a análise\n",
    "df_pivotado = df_pesquisa.groupBy(\"ano\", \"municipio\") \\\n",
    "    .pivot(\"pesquisa\") \\\n",
    "    .agg({\"valor\": \"first\"})\n",
    "\n",
    "# Seleção das colunas relevantes para a análise final\n",
    "df_pesquisa_final = df_pivotado.select(\n",
    "    col('ano'),\n",
    "    col('municipio'),\n",
    "    col('idh').astype(DoubleType()),  # Convertendo para tipo Double\n",
    "    col('pib').astype(DoubleType()),  # Convertendo para tipo Double\n",
    "    col('populacional').astype(IntegerType())  # Convertendo para tipo Integer\n",
    ")\n",
    "\n",
    "# Calcular quartis da coluna de população\n",
    "quartis = df_pesquisa_final.approxQuantile(\"populacional\", [0.25, 0.5, 0.75], 0.01)\n",
    "\n",
    "# Definir os limites dos quadrantes com base nos quartis\n",
    "limite_quadrante_1 = quartis[0]\n",
    "limite_quadrante_2 = quartis[1]\n",
    "limite_quadrante_3 = quartis[2]\n",
    "limite_quadrante_4 = float(\"inf\")  # O último quadrante vai até o infinito\n",
    "\n",
    "# Adicionar uma coluna \"Faixa de População\" ao DataFrame com base nos limites dos quadrantes\n",
    "df_pesquisa_final = df_pesquisa_final.withColumn(\"Faixa de População\",\n",
    "                                                when(col(\"populacional\") <= limite_quadrante_1, f\"Entre 0 e {int(limite_quadrante_1)}\")\n",
    "                                                .when((col(\"populacional\") > limite_quadrante_1) & (col(\"populacional\") <= limite_quadrante_2), f\"Entre {int(limite_quadrante_1)} e {int(limite_quadrante_2)}\")\n",
    "                                                .when((col(\"populacional\") > limite_quadrante_2) & (col(\"populacional\") <= limite_quadrante_3), f\"Entre {int(limite_quadrante_2)} e {int(limite_quadrante_3)}\")\n",
    "                                                .when((col(\"populacional\") > limite_quadrante_3) & (col(\"populacional\") <= limite_quadrante_4), f\"Maior que {int(limite_quadrante_3)}\")\n",
    "                                                .otherwise(\"Outro\"))\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "df_pesquisa_final.show()\n",
    "\n",
    "# Escrever os resultados no banco de dados 'gold_data'\n",
    "df_pesquisa_final.write.format(\"jdbc\").option(\"url\", gold_url) \\\n",
    "    .option(\"user\", \"ibge\") \\\n",
    "    .option(\"password\", \"ibge\") \\\n",
    "    .option(\"dbtable\", \"public.fato_pesquisa\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315c6e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
