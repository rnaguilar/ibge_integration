{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c19eb0",
   "metadata": {},
   "source": [
    "# Instanciar Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f9578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, from_json, explode, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "PATH = \"C:\\\\PROJETOS\\\\ibge_integration\\\\RAW_DATA\\\\\"  # SUBSTITUA UM PATH PARA UM DE SUA PREFERÊNCIA OU UM BUCKET\n",
    "PATH = \"E:\\\\AMBIENTE DEV\\\\PROJETOS\\\\ibge_integration\\\\RAW_DATA\\\\\"  # SUBSTITUA UM PATH PARA UM DE SUA PREFERÊNCIA OU UM BUCKET\n",
    "\n",
    "# Configuração da Spark Session\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"Ibge\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e5f84",
   "metadata": {},
   "source": [
    "# SQL CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c85383ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações de conexão com o PostgreSQL\n",
    "# Este item é totalmente editavel, para uma conexão de sua preferencia (Vamos fazer conforme o docker-compose que subimos)\n",
    "url_database = \"jdbc:postgresql://localhost:8085/bronze_data\"\n",
    "properties = {\"user\": \"ibge\", \"password\": \"ibge\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# engine = create_engine(url_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c9cd3",
   "metadata": {},
   "source": [
    "# BRONZE DATA - (Região, Estado e Municipio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d6ea3ab-bfb1-40ab-872c-a167949231e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LER O PARQUE DE REGIÕES E GRAVAR NO BANCO\n",
    "\n",
    "# Ler o arquivo Parquet de regiões\n",
    "df_regions = spark_session.read.parquet(f\"{PATH}\\\\regioes.parquet\")\n",
    "\n",
    "# Renomear as colunas para adequar aos nomes do banco de dados\n",
    "df_regions = df_regions.withColumnRenamed(\"id\", \"id_regiao\").withColumnRenamed(\"nome\", \"regiao\")\n",
    "\n",
    "# Escrever os dados no PostgreSQL usando SQL Alchemy e JDBC\n",
    "df_regions.write \\\n",
    "    .jdbc(url=url_database, table=\"regiao\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a074f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LER O PARQUE DE ESTADO E GRAVAR NO BANCO\n",
    "\n",
    "# Ler o arquivo Parquet de estados\n",
    "df_state = spark_session.read.parquet(f\"{PATH}\\\\estados.parquet\")\n",
    "\n",
    "# Renomear as colunas para adequar aos nomes do banco de dados\n",
    "df_state = df_state.withColumnRenamed(\"id\", \"id_estado\").withColumnRenamed(\"sigla\", \"uf\").withColumnRenamed(\"nome\", \"estado\")\n",
    "\n",
    "# Selecionar as colunas necessárias e renomear a coluna da região\n",
    "df_state_select = df_state.select(\n",
    "    'id_estado',\n",
    "    'uf',\n",
    "    'estado',\n",
    "    col('regiao.id').alias('id_regiao')\n",
    ")\n",
    "\n",
    "# Escrever os dados no PostgreSQL\n",
    "df_state_select.write \\\n",
    "    .jdbc(url=url_database, table=\"estado\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b6df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LER O PARQUE DE MUNICÍPIO E GRAVAR NO BANCO\n",
    "\n",
    "# Ler o arquivo Parquet de municípios\n",
    "df_substate = spark_session.read.parquet(f\"{PATH}\\\\municipios.parquet\")\n",
    "\n",
    "# Renomear as colunas para adequar aos nomes do banco de dados\n",
    "df_substate = df_substate.withColumnRenamed(\"ibge\", \"id_municipio\").withColumnRenamed(\"nome\", \"municipio\")\n",
    "\n",
    "# Escrever os dados no PostgreSQL\n",
    "df_substate.write \\\n",
    "    .jdbc(url=url_database, table=\"municipios\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162e7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LER UM CSV COM AS CAPITAIS\n",
    "\n",
    "# Ler o arquivo CSV das capitais\n",
    "df_capital = spark_session.read.csv(f\"{PATH}\\\\capital.csv\", header=True, inferSchema=True, sep=\";\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Selecionar as colunas relevantes e renomeá-las\n",
    "df_capital = df_capital.select(\n",
    "    col(\"Estados\").alias(\"estado\"),\n",
    "    col(\"Capital\").alias(\"capital\"),\n",
    "    col(\"Sigla\").alias(\"uf\")\n",
    ")\n",
    "\n",
    "# Escrever os dados no PostgreSQL\n",
    "df_capital.write \\\n",
    "    .jdbc(url=url_database, table=\"capitais\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e50b3",
   "metadata": {},
   "source": [
    "# PESQUISAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b2ae729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    }
   ],
   "source": [
    "# LER A PESQUISA IDH\n",
    "\n",
    "# Define o subdiretório e o caminho da pasta\n",
    "subfolder = 'idh'\n",
    "pathstring = f'{PATH}\\\\{subfolder}\\\\'\n",
    "\n",
    "# Lista todos os arquivos Parquet na pasta\n",
    "arquivos_parquet = [os.path.join(pathstring, arquivo) for arquivo in os.listdir(pathstring) if arquivo.endswith(\".parquet\")]\n",
    "\n",
    "# Lê todos os arquivos Parquet e cria um DataFrame único\n",
    "df_idh = spark_session.read.parquet(*arquivos_parquet)\n",
    "\n",
    "# Exibe a contagem de linhas no DataFrame\n",
    "print(df_idh.count())\n",
    "\n",
    "# Escreve os dados no PostgreSQL \n",
    "df_idh.write \\\n",
    "    .jdbc(url=url_database, table=\"pesquisas_idh\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee89d8dc-0fb3-4bcf-b91d-7f544544c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22280\n"
     ]
    }
   ],
   "source": [
    "# LER A PESQUISA PIB\n",
    "\n",
    "# Define o subdiretório e o caminho da pasta\n",
    "subfolder = 'pib'\n",
    "pathstring = f'{PATH}\\\\{subfolder}\\\\'\n",
    "\n",
    "# Lista todos os arquivos Parquet na pasta\n",
    "arquivos_parquet = [os.path.join(pathstring, arquivo) for arquivo in os.listdir(pathstring) if arquivo.endswith(\".parquet\")]\n",
    "\n",
    "# Lê todos os arquivos Parquet e cria um DataFrame único\n",
    "df_pib = spark_session.read.parquet(*arquivos_parquet)\n",
    "\n",
    "# Exibe a contagem de linhas no DataFrame\n",
    "print(df_pib.count())\n",
    "\n",
    "# Escreve os dados no PostgreSQL \n",
    "df_pib.write \\\n",
    "    .jdbc(url=url_database, table=\"pesquisas_pib\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7da358ad-51b6-4bc3-8f24-bceae1bd8fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22280\n"
     ]
    }
   ],
   "source": [
    "# LER A PESQUISA POPULAÇÃO\n",
    "\n",
    "# Define o subdiretório e o caminho da pasta\n",
    "subfolder = 'populacao'\n",
    "pathstring = f'{PATH}\\\\{subfolder}\\\\'\n",
    "\n",
    "# Lista todos os arquivos Parquet na pasta\n",
    "arquivos_parquet = [os.path.join(pathstring, arquivo) for arquivo in os.listdir(pathstring) if arquivo.endswith(\".parquet\")]\n",
    "\n",
    "# Lê todos os arquivos Parquet e cria um DataFrame único\n",
    "df_populacao = spark_session.read.parquet(*arquivos_parquet)\n",
    "\n",
    "# Exibe a contagem de linhas no DataFrame\n",
    "print(df_populacao.count())\n",
    "\n",
    "# Escreve os dados no PostgreSQL \n",
    "df_populacao.write \\\n",
    "    .jdbc(url=url_database, table=\"pesquisas_populacao\", mode=\"overwrite\", properties=properties)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
